<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What's Left</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
<nav>
    <div class="nav-inner">
        <a href="../index.html" class="site-title">~ failing at ml research</a>
        <a href="../index.html">posts</a>
        <a href="about.html">about</a>
    </div>
</nav>
<div class="container">

<h1>What's Left</h1>
<p class="post-date">February 2026</p>
<p class="subtitle">Three threads that looked promising. Two are now dead. One remains open but unvalidated.</p>

<div class="finding">
    <div class="finding-header">
        <h3 style="margin:0">Rank-Aware Muon</h3>
        <span class="tag tag-dead">Dead</span>
    </div>
    <p><strong>The observation:</strong> Value projection gradients were claimed to be rank-1. MLP gradients rank 3-8. lm_head gradients rank 15.7. But Muon orthogonalizes everything identically into full-rank updates via Newton-Schulz.</p>
    <p><strong>The idea:</strong> Adapt NS iterations based on measured gradient rank. Low-rank layers get fewer iterations (closer to Adam). High-rank layers get full orthogonalization.</p>
    <p><strong>What happened:</strong> Tested on both 5M toy model and 73M nanochat (6 layers, dim=384, batch=8, 1000 steps).</p>
    <ul>
        <li><strong>5M model:</strong> All gradient entropy ranks were high (82-243 out of 256). No low-rank layers exist. Variable NS (0/2/5 and 1/3/5 by rank) gave identical results to NS=5. Best variant (attn.proj&rarr;Adam) improved 0.013 (0.4%), within noise.</li>
        <li><strong>73M model, single-batch:</strong> Lowest entropy rank ~50 for value projections at step 300. Still far from rank-1. No low-rank layers found.</li>
        <li><strong>73M model, 20-batch averaged:</strong> Averaging reduces noise and reveals lower rank: c_v drops to entropy rank 22-32 with stable rank 1.08-1.21. One direction dominates (top-1 energy 82-92%), but entropy rank captures a significant tail. This is "concentrated" but not "rank-1."</li>
        <li><strong>Last layer MLP:</strong> h.5.mlp.c_fc showed entropy rank 19.7, stable rank 1.03, top-1 energy 97.2% &mdash; the most rank-1-like gradient in the entire model, and it's not a value projection.</li>
    </ul>
    <p><strong>Verdict:</strong> The premise was wrong. At batch=8, no layer has rank-1 gradients. Mini-batch noise inflates the apparent rank 2-3x. Even the clean 20-batch signal shows entropy rank 20-30, not 1. The "rank-1 value projection" claim may be a batch-size artifact (nanochat default total_batch=16384), but we can't test that on a Mac Mini.</p>
</div>

<div class="finding">
    <div class="finding-header">
        <h3 style="margin:0">Weight Decay Hurts Muon</h3>
        <span class="tag tag-open">Open</span>
    </div>
    <p><strong>The observation:</strong> On 73M nanochat, removing weight decay improved loss from 4.97 to 4.90. Small (0.07) but consistent across runs.</p>
    <p><strong>The hypothesis:</strong> Muon already produces well-conditioned updates (condition number 6-10). Weight decay's main benefit is regularization (preventing large weights / ill-conditioned updates). Since Muon inherently prevents ill-conditioned updates through orthogonalization, weight decay is redundant &mdash; and the parameter shrinkage it causes is actively harmful, pulling weights back toward initialization when they should be moving far away (Muon travels 59,000+ units from init).</p>
    <p><strong>Status:</strong> Observed on one model (nanochat, 1000 steps). Needs validation at longer training and different scales. Could be a training-length artifact.</p>
</div>

<div class="finding">
    <div class="finding-header">
        <h3 style="margin:0">Value Projection is Special</h3>
        <span class="tag tag-dead">Dead</span>
    </div>
    <p><strong>The observation:</strong> c_v (value projection) layers have the lowest gradient rank of any layer type.</p>
    <p><strong>The hypothesis:</strong> Value projections might benefit from Adam instead of Muon, since their low-rank gradients mean Muon amplifies mostly noise.</p>
    <p><strong>What happened:</strong> On the 5M model, moving attn.proj to Adam improved loss by 0.013 (0.4%, within noise). On the 73M model, value projection gradients had entropy rank 22-32 (multi-batch averaged) &mdash; concentrated but not pathologically low-rank. The gap between c_v and other layers is real but smaller than originally thought, and the intervention doesn't help.</p>
    <p><strong>Verdict:</strong> Merged into the rank-aware Muon dead end. Value projections are somewhat special (lower rank, higher top-1 energy) but not special enough to warrant a different optimizer.</p>
</div>

<h2>The honest summary</h2>

<p>Every "clever trick" either reduces to something simple that already exists (SpectraMuon is an LR change, planning is worse beam search) or the signal isn't there (curvature, conservation laws, line search, rank-aware Muon).</p>

<p>The measurements themselves are genuine. The optimizer fingerprints, the scaling laws, the landscape geometry, the gradient rank structure &mdash; these are real findings that describe how transformer training actually works. They just haven't led to a better optimizer.</p>

<p>The only remaining lead is weight decay interaction with Muon. It's a small effect (0.07 loss improvement) observed at one scale. It might be real. It might be a training-length artifact. Testing it properly requires longer runs at larger scale &mdash; more compute than a Mac Mini can provide.</p>

<div class="page-nav">
    <a href="muzero-lm.html">&larr; Prev: MuZero for Language</a>
    <span></span>
</div>

<footer>
    <p>Built with real experiments, not theory.<br>February 2026</p>
</footer>

</div>
</body>
</html>
